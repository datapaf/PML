{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WpIeeeM1J6u"
      },
      "source": [
        "# ELMo: Embeddings from Language Models \n",
        "<!-- ![](https://get.whotrades.com/u4/photoDE6C/20647654315-0/blogpost.jpeg) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjrI02kE2Zfr"
      },
      "source": [
        "In this assignment you will implement a deep lstm-based model for contextualized word embeddings - ELMo. Your tasks are as following: \n",
        "\n",
        "- Preprocessing (20 points)\n",
        "- Implementation of ELMo model (30 points)\n",
        "  - 2-layer BiLSTM (15 points)\n",
        "  - Highway layers (5 points) [link](https://paperswithcode.com/method/highway-layer) [paper](https://arxiv.org/pdf/1507.06228.pdf) [code](https://github.com/allenai/allennlp/blob/9f879b0964e035db711e018e8099863128b4a46f/allennlp/modules/highway.py#L11)\n",
        "  - CharCNN embeddings (5 points) [paper](https://arxiv.org/pdf/1509.01626.pdf)\n",
        "  - Handle out-of-vocabulary words (5 points)\n",
        "- Report metrics and loss using tensorbord/comet or other tool.  (10 points)\n",
        "- Evaluate on movie review dataset (20 pts)\n",
        "- Compare the performance with BERT model (10 pts)\n",
        "- Clean and documented code (10 points)\n",
        "\n",
        "\n",
        "Remarks: \n",
        "\n",
        "*   Use Pytorch\n",
        "*   Cheating will result in 0 points\n",
        "\n",
        "\n",
        "ELMo paper: https://arxiv.org/pdf/1802.05365.pdf\n",
        "\n",
        "Possible datasets:\n",
        "- [WikiText-103](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/)\n",
        "- Any monolingual dataset from [WMT](https://statmt.org/wmt22/translation-task.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoFWzNeaTOTu"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "Preprocess the english monolingual data (20 points):\n",
        "- clean\n",
        "- split to train and validation\n",
        "- tokenize\n",
        "- create vocabulary, convert words to numbers. [vocab](https://pytorch.org/text/stable/vocab.html#id1)\n",
        "- pad sequences\n",
        "\n",
        "Use these tutorials [one](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html) and [two](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) as a reference\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*UPirqwpBWnNmcwoUjfZZIA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       ' 1979 standup tour. citation In 1998, he rele...\n",
              "1       A 100-year-old woman named Rose DeWitt Bukater...\n",
              "2       A1 is the name of a major road in some countries.\n",
              "3       A 2002 report by American Sports Data found th...\n",
              "4                   A 268-page booklet available on-line.\n",
              "                              ...                        \n",
              "9555    Zones are the places where buildings can develop.\n",
              "9556    Zoological Journal of the Linnean Society, 71,...\n",
              "9557    Zou Tribe is one of the Schedule Tribes of Man...\n",
              "9558    Zubeyr was killed in a U.S. drone airstrike on...\n",
              "9559    Քաշաթաղի մելիքություն) - Armenian melikdom(pri...\n",
              "Name: 1, Length: 9560, dtype: object"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os \n",
        "\n",
        "data_dir = 'eng-simple_wikipedia_2021_10K'\n",
        "data_filename = \"eng-simple_wikipedia_2021_10K-sentences.txt\"\n",
        "data_full_filename = os.path.join(data_dir, data_filename)\n",
        "\n",
        "# read data_ful_filename into a pandas dataframe without index\n",
        "sents = pd.read_csv(data_full_filename, sep='\\t', header=None, index_col=False)[1]\n",
        "sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       ' 1979 standup tour. citation In 1998, he rele...\n",
              "1       A 100-year-old woman named Rose DeWitt Bukater...\n",
              "2       A1 is the name of a major road in some countries.\n",
              "3       A 2002 report by American Sports Data found th...\n",
              "4                   A 268-page booklet available on-line.\n",
              "                              ...                        \n",
              "9554                                  Z is not used much.\n",
              "9555    Zones are the places where buildings can develop.\n",
              "9556    Zoological Journal of the Linnean Society, 71,...\n",
              "9557    Zou Tribe is one of the Schedule Tribes of Man...\n",
              "9558    Zubeyr was killed in a U.S. drone airstrike on...\n",
              "Name: 1, Length: 8969, dtype: object"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ascii_sent_indices = np.array(list(map(lambda x: x.isascii(), sents)))\n",
        "ascii_sents = sents[ascii_sent_indices]\n",
        "ascii_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split sentences into train and test sets with numpy\n",
        "np.random.seed(42)\n",
        "train_indices = np.random.choice(ascii_sents.index, size=int(0.8 * len(ascii_sents)), replace=False)\n",
        "test_indices = ascii_sents.index.difference(train_indices)\n",
        "train_sents = ascii_sents.loc[train_indices]\n",
        "test_sents = ascii_sents.loc[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6_05j-_TR76"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchtext.vocab import vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU_JDeZ6TTx2"
      },
      "source": [
        "## Model - learning embeddings\n",
        "Read chapter 3 from the [paper](https://arxiv.org/pdf/1802.05365.pdf)\n",
        "\n",
        "Implement this model with \n",
        "- 2 BiLSTM layers,\n",
        "- CharCNN embeddings,\n",
        "- Highway layers,\n",
        "- out-of-vocabulary words handling\n",
        "\n",
        "Plot the training and validation losses over the epochs (iterations)\n",
        "\n",
        "Use the [implementation](https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py) as a reference\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*3_wsDpyNG-TylsRACF48yA.png)\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*8pG54o28pbD2L0dv5THL-A.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUr4mW-J8V5o"
      },
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ4HsuafA5sQ"
      },
      "source": [
        "## Evaluate your embeddings model on IMDB movie reviews dataset (sentiment analysis) \n",
        "[Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "\n",
        "Preprocess data\n",
        "\n",
        "Disable training for ELMo, it will produce 5 embeddings for each word, add trainable parameters $\\gamma^{task}$ and $s^{task}_j$\n",
        "\n",
        "Don't forget metric plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ_0LTQf81CM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKhTvahJBcBI"
      },
      "source": [
        "## Compare the results with BERT embeddings\n",
        "you can choose other bert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCojr57Zov7t"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "454c98ae2731e865a69a0b883f25a1cfa6b0f63785a62bbc0572ffd435d4c747"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
