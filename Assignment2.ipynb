{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WpIeeeM1J6u"
      },
      "source": [
        "# ELMo: Embeddings from Language Models \n",
        "<!-- ![](https://get.whotrades.com/u4/photoDE6C/20647654315-0/blogpost.jpeg) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjrI02kE2Zfr"
      },
      "source": [
        "In this assignment you will implement a deep lstm-based model for contextualized word embeddings - ELMo. Your tasks are as following: \n",
        "\n",
        "- Preprocessing (20 points)\n",
        "- Implementation of ELMo model (30 points)\n",
        "  - 2-layer BiLSTM (15 points)\n",
        "  - Highway layers (5 points) [link](https://paperswithcode.com/method/highway-layer) [paper](https://arxiv.org/pdf/1507.06228.pdf) [code](https://github.com/allenai/allennlp/blob/9f879b0964e035db711e018e8099863128b4a46f/allennlp/modules/highway.py#L11)\n",
        "  - CharCNN embeddings (5 points) [paper](https://arxiv.org/pdf/1509.01626.pdf)\n",
        "  - Handle out-of-vocabulary words (5 points)\n",
        "- Report metrics and loss using tensorbord/comet or other tool.  (10 points)\n",
        "- Evaluate on movie review dataset (20 pts)\n",
        "- Compare the performance with BERT model (10 pts)\n",
        "- Clean and documented code (10 points)\n",
        "\n",
        "\n",
        "Remarks: \n",
        "\n",
        "*   Use Pytorch\n",
        "*   Cheating will result in 0 points\n",
        "\n",
        "\n",
        "ELMo paper: https://arxiv.org/pdf/1802.05365.pdf\n",
        "\n",
        "Possible datasets:\n",
        "- [WikiText-103](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/)\n",
        "- Any monolingual dataset from [WMT](https://statmt.org/wmt22/translation-task.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoFWzNeaTOTu"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "Preprocess the english monolingual data (20 points):\n",
        "- clean\n",
        "- split to train and validation\n",
        "- tokenize\n",
        "- create vocabulary, convert words to numbers. [vocab](https://pytorch.org/text/stable/vocab.html#id1)\n",
        "- pad sequences\n",
        "\n",
        "Use these tutorials [one](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html) and [two](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) as a reference\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*UPirqwpBWnNmcwoUjfZZIA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\datapaf\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       ' 1979 standup tour. citation In 1998, he rele...\n",
              "1       A 100-year-old woman named Rose DeWitt Bukater...\n",
              "2       A1 is the name of a major road in some countries.\n",
              "3       A 2002 report by American Sports Data found th...\n",
              "4                   A 268-page booklet available on-line.\n",
              "                              ...                        \n",
              "9555    Zones are the places where buildings can develop.\n",
              "9556    Zoological Journal of the Linnean Society, 71,...\n",
              "9557    Zou Tribe is one of the Schedule Tribes of Man...\n",
              "9558    Zubeyr was killed in a U.S. drone airstrike on...\n",
              "9559    Քաշաթաղի մելիքություն) - Armenian melikdom(pri...\n",
              "Name: 1, Length: 9560, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_dir = 'eng-simple_wikipedia_2021_10K'\n",
        "data_filename = \"eng-simple_wikipedia_2021_10K-sentences.txt\"\n",
        "data_full_filename = os.path.join(data_dir, data_filename)\n",
        "\n",
        "# read data_ful_filename into a pandas dataframe without index\n",
        "sents = pd.read_csv(data_full_filename, sep='\\t', header=None, index_col=False)[1]\n",
        "sents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter ASCII-only Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       ' 1979 standup tour. citation In 1998, he rele...\n",
              "1       A 100-year-old woman named Rose DeWitt Bukater...\n",
              "2       A1 is the name of a major road in some countries.\n",
              "3       A 2002 report by American Sports Data found th...\n",
              "4                   A 268-page booklet available on-line.\n",
              "                              ...                        \n",
              "8964                                  Z is not used much.\n",
              "8965    Zones are the places where buildings can develop.\n",
              "8966    Zoological Journal of the Linnean Society, 71,...\n",
              "8967    Zou Tribe is one of the Schedule Tribes of Man...\n",
              "8968    Zubeyr was killed in a U.S. drone airstrike on...\n",
              "Name: 1, Length: 8969, dtype: object"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ascii_sent_indices = np.array(list(map(lambda x: x.isascii(), sents)))\n",
        "ascii_sents = sents[ascii_sent_indices]\n",
        "ascii_sents = ascii_sents.reset_index(drop=True)\n",
        "ascii_sents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train-Test Indices (80% - train, 20% - test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split sentences into train and test sets with numpy\n",
        "np.random.seed(42)\n",
        "train_indices = np.random.choice(ascii_sents.index, size=int(0.8*len(ascii_sents)), replace=False)\n",
        "test_indices = ascii_sents.index.difference(train_indices)\n",
        "train_sents = ascii_sents.loc[train_indices]\n",
        "test_sents = ascii_sents.loc[test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# create pytorch tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create vocabulary of training words\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    [tokenizer(sent) for sent in train_sents],\n",
        "    specials=['<unk>', '<pad>', '<bos>', '<eos>']\n",
        ")\n",
        "\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Char Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create vocabulary of ascii symbols\n",
        "ascii_symbols = list(map(chr, range(127)))\n",
        "\n",
        "symbols_vocab = build_vocab_from_iterator(\n",
        "    [ascii_symbols],\n",
        "    specials=['<unk>', '<pad>', '<bow>', '<eow>']\n",
        ")\n",
        "\n",
        "symbols_vocab.set_default_index(symbols_vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenized Sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenize ascii_sents\n",
        "tokenized_sents = list(map(tokenizer, ascii_sents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Max Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "571"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get max number of words in tokenized_sents\n",
        "max_num_words = max(map(lambda x: len(x), tokenized_sents))\n",
        "max_num_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Max Letters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get max number of letters in the words in tokenized_sents\n",
        "max_num_letters = max(map(lambda x: max(map(lambda y: len(y), x)), tokenized_sents))\n",
        "max_num_letters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Padded Word Ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sents_to_word_ids(sents):\n",
        "    word_ids = []\n",
        "    for sent in sents:\n",
        "        sent_word_ids = torch.tensor([vocab['<bos>']] + [vocab[token] for token in tokenizer(sent)] + [vocab['<eos>']])\n",
        "        word_ids.append(sent_word_ids)\n",
        "    return word_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([    2,    18,  1187, 14913,   979,     4,   104,     8,   689,     6,\n",
              "            15,   103,  4593,  1159,     6,    10,  2233,   181,     4,     3]),\n",
              " tensor([    2,    10,  6603,   388,   178,  1826,  9339,  8270,  1067,    10,\n",
              "           302,    63,    49, 15982,    19,     5,   307,  1166, 15466,     4,\n",
              "             3])]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_ids = sents_to_word_ids(ascii_sents)\n",
        "word_ids[:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[    2,    18,  1187,  ...,     1,     1,     1],\n",
              "        [    2,    10,  6603,  ...,     1,     1,     1],\n",
              "        [    2,  7135,    12,  ...,     1,     1,     1],\n",
              "        ...,\n",
              "        [    2,     0,  1763,  ...,     1,     1,     1],\n",
              "        [    2,  6560,  3163,  ...,     1,     1,     1],\n",
              "        [    2, 16338,    13,  ...,     1,     1,     1]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_word_ids = torch.nn.utils.rnn.pad_sequence(word_ids, padding_value=vocab['<pad>'], batch_first=True)\n",
        "padded_word_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8969, 573])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_word_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# padded_word_ids = padded_word_ids.to(device)\n",
        "# padded_word_ids.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BATCH_SIZE = 128\n",
        "\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# def pad_batch(data_batch):\n",
        "#     return pad_sequence(data_batch, padding_value=vocab['<pad>'])\n",
        "\n",
        "# train_iter = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
        "# test_iter = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Padded Char Ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8969, 573, 35])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_char_ids = torch.full(\n",
        "    size=(len(ascii_sents), max_num_words + 2, max_num_letters),\n",
        "    fill_value=symbols_vocab['<pad>']\n",
        ")\n",
        "padded_char_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "for sent_num, sent in enumerate(ascii_sents):\n",
        "    tokenized = tokenizer(sent)\n",
        "    for i in range(0, len(tokenized) + 2):\n",
        "        if i == 0:\n",
        "            padded_char_ids[sent_num, i, 0] = symbols_vocab['<bow>']\n",
        "            padded_char_ids[sent_num, i, 1] = symbols_vocab['<bos>']\n",
        "            padded_char_ids[sent_num, i, 2] = symbols_vocab['<eow>']\n",
        "            continue\n",
        "        elif i == len(tokenized) + 1:\n",
        "            padded_char_ids[sent_num, i, 0] = symbols_vocab['<bow>']\n",
        "            padded_char_ids[sent_num, i, 1] = symbols_vocab['<eos>']\n",
        "            padded_char_ids[sent_num, i, 2] = symbols_vocab['<eow>']\n",
        "            continue\n",
        "        word = tokenized[i - 1]\n",
        "        for letter_num, letter in enumerate(word):\n",
        "            padded_char_ids[sent_num, i, letter_num] = symbols_vocab[letter]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "symbols_vocab['.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[  2,   0,   3,  ...,   1,   1,   1],\n",
              "         [ 43,   1,   1,  ...,   1,   1,   1],\n",
              "         [ 53,  61,  59,  ...,   1,   1,   1],\n",
              "         ...,\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1]],\n",
              "\n",
              "        [[  2,   0,   3,  ...,   1,   1,   1],\n",
              "         [101,   1,   1,  ...,   1,   1,   1],\n",
              "         [ 53,  52,  52,  ...,   1,   1,   1],\n",
              "         ...,\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1]],\n",
              "\n",
              "        [[  2,   0,   3,  ...,   1,   1,   1],\n",
              "         [101,  53,   1,  ...,   1,   1,   1],\n",
              "         [109, 119,   1,  ...,   1,   1,   1],\n",
              "         ...,\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[  2,   0,   3,  ...,   1,   1,   1],\n",
              "         [126, 115, 115,  ...,   1,   1,   1],\n",
              "         [110, 115, 121,  ...,   1,   1,   1],\n",
              "         ...,\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1]],\n",
              "\n",
              "        [[  2,   0,   3,  ...,   1,   1,   1],\n",
              "         [126, 115, 121,  ...,   1,   1,   1],\n",
              "         [120, 118, 109,  ...,   1,   1,   1],\n",
              "         ...,\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1]],\n",
              "\n",
              "        [[  2,   0,   3,  ...,   1,   1,   1],\n",
              "         [126, 121, 102,  ...,   1,   1,   1],\n",
              "         [123, 101, 119,  ...,   1,   1,   1],\n",
              "         ...,\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1],\n",
              "         [  1,   1,   1,  ...,   1,   1,   1]]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_char_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8969, 573, 35])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_char_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# padded_char_ids = padded_char_ids.to(device)\n",
        "# padded_char_ids.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "f6_05j-_TR76"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import TensorDataset, DataLoader\n",
        "# from torchtext.vocab import vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataset from padded_word_ids and padded_char_ids\n",
        "\n",
        "# import TensorDataset\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "train_ds = TensorDataset(padded_word_ids[train_indices], padded_char_ids[train_indices])\n",
        "test_ds = TensorDataset(padded_word_ids[test_indices], padded_char_ids[test_indices])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iter = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_iter = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU_JDeZ6TTx2"
      },
      "source": [
        "## Model - learning embeddings\n",
        "Read chapter 3 from the [paper](https://arxiv.org/pdf/1802.05365.pdf)\n",
        "\n",
        "Implement this model with \n",
        "- 2 BiLSTM layers,\n",
        "- CharCNN embeddings,\n",
        "- Highway layers,\n",
        "- out-of-vocabulary words handling\n",
        "\n",
        "Plot the training and validation losses over the epochs (iterations)\n",
        "\n",
        "Use the [implementation](https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py) as a reference\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*3_wsDpyNG-TylsRACF48yA.png)\n",
        "\n",
        "![](https://miro.medium.com/max/720/1*8pG54o28pbD2L0dv5THL-A.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class ELMo(nn.Module):\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        n_tokens,\n",
        "        n_chars=50,\n",
        "        embedding_dim=4,\n",
        "        lstm_units=256,\n",
        "        elmo_output_size=32\n",
        "    ):\n",
        "        super(ELMo, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_tokens = n_tokens\n",
        "        self.n_chars = n_chars\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lstm_units = lstm_units\n",
        "        self.elmo_output_size = elmo_output_size\n",
        "        \n",
        "\n",
        "        self.embedding_matrix = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        filters = [[1,4], [2,8], [3,26], [4,32], [5,64]]\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                in_channels=4,\n",
        "                out_channels=num,\n",
        "                kernel_size=width,\n",
        "                bias=True \n",
        "            )\n",
        "            for (width, num) in filters\n",
        "        ])\n",
        "        self.conv_activation = nn.ReLU()\n",
        "\n",
        "        self.highway_layers = nn.ModuleList([\n",
        "            nn.Linear(134, 134 * 2)\n",
        "            for _ in range(2)\n",
        "        ])\n",
        "        self.highway_activation = nn.ReLU()\n",
        "        self.highway_projection = nn.Linear(134, elmo_output_size, bias=True)\n",
        "        \n",
        "        self.lstm1 = nn.LSTM(\n",
        "            input_size=elmo_output_size,\n",
        "            hidden_size=lstm_units,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            proj_size=elmo_output_size\n",
        "        )\n",
        "\n",
        "        self.lstm2 = nn.LSTM(\n",
        "            input_size=2*elmo_output_size,\n",
        "            hidden_size=lstm_units,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            proj_size=elmo_output_size\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Linear(2 * elmo_output_size, vocab_size, bias=True)\n",
        "\n",
        "    def embed_input(self, x):\n",
        "        return self.embedding_matrix(x.view(-1, self.n_chars))\n",
        "\n",
        "    def charCNN(self, x):\n",
        "        embedded = torch.transpose(x, 1, 2)\n",
        "\n",
        "        # pass the embedded input through the convolutional layers\n",
        "        conv_outputs = []\n",
        "        for conv_layer in self.conv_layers:\n",
        "            conv_output = conv_layer(embedded)\n",
        "            conv_output, _ = torch.max(conv_output, dim=-1)\n",
        "            conv_output = self.conv_activation(conv_output)\n",
        "            conv_outputs.append(conv_output)\n",
        "\n",
        "        # concatenate the conv outputs\n",
        "        token_embedding = torch.cat(conv_outputs, dim=-1)\n",
        "\n",
        "        # pass the conv output through the highway layers\n",
        "        highway_output = token_embedding\n",
        "        for highway_layer in self.highway_layers:\n",
        "            projected_input = highway_layer(highway_output)\n",
        "            linear_part = highway_output\n",
        "\n",
        "            nonlinear_part, gate = projected_input.chunk(2, dim=-1)\n",
        "            nonlinear_part = self.highway_activation(nonlinear_part)\n",
        "            gate = torch.sigmoid(gate)\n",
        "\n",
        "            highway_output = gate * linear_part + (1 - gate) * nonlinear_part\n",
        "        \n",
        "        token_embedding = self.highway_projection(highway_output) \n",
        "\n",
        "        return token_embedding\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #batch_size = x.size(0)\n",
        "\n",
        "        # embed the input\n",
        "        embedded = self.embed_input(x)\n",
        "\n",
        "        # CharCNN\n",
        "        token_embedding = self.charCNN(embedded)\n",
        "\n",
        "        # pass the token embedding through the BiLSTM\n",
        "        lstm_output1, (h1_n, c1_n) = self.lstm1(token_embedding)\n",
        "        lstm_output2, (h2_n, c2_n) = self.lstm2(lstm_output1)\n",
        "\n",
        "        out = self.linear(lstm_output2)\n",
        "        \n",
        "        return out.view(-1, self.n_tokens+2, self.vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ELMo(vocab_size=len(vocab), n_tokens=max_num_words, n_chars=max_num_letters).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EPOCH 0 ===\n",
            "Training loss: 9.832897186279297, accuracy: 0.0\n",
            "Training loss: 9.302080414511941, accuracy: 0.35220331588132636\n",
            "Training loss: 5.541064239683605, accuracy: 0.6460462893708967\n",
            "Training loss: 3.959418814028463, accuracy: 0.7495918482238361\n",
            "Training loss: 3.125066915663277, accuracy: 0.802962584599668\n",
            "Training loss: 2.596915250315386, accuracy: 0.835628186702255\n",
            "Training loss: 2.2433411640221954, accuracy: 0.85694003375962\n",
            "Training loss: 1.976062471597967, accuracy: 0.8729905611680554\n",
            "Training loss: 1.7755501715489377, accuracy: 0.8849339086031931\n",
            "Training loss: 1.6287861387808245, accuracy: 0.8933461826131983\n",
            "Training loss: 1.5028183457284872, accuracy: 0.9008216266652843\n",
            "Training loss: 1.4006110988221727, accuracy: 0.9068332075531028\n",
            "Training loss: 1.3144694069200311, accuracy: 0.911925958778648\n",
            "Training loss: 1.241726153451978, accuracy: 0.9162145464476507\n",
            "Training loss: 1.1788303209534774, accuracy: 0.9199188048469545\n",
            "Training loss: 1.124822444671037, accuracy: 0.9230768119459566\n",
            "Training loss: 1.0765843297013584, accuracy: 0.9259238181956142\n",
            "Training loss: 1.0359837254237014, accuracy: 0.9282509465927763\n",
            "Training loss: 0.998542702659059, accuracy: 0.9304372402688188\n",
            "Training loss: 0.9639771003373631, accuracy: 0.9325008223458787\n",
            "Training loss: 0.9349096304741665, accuracy: 0.9341680558811527\n",
            "Training loss: 0.9078761634385981, accuracy: 0.9357361893418691\n",
            "Training loss: 0.884174532210665, accuracy: 0.9370839354670584\n",
            "Training loss: 0.8607806888493624, accuracy: 0.938472231665949\n",
            "Training loss: 0.8397531619457783, accuracy: 0.9396914579305251\n",
            "Training loss: 0.8197438484169097, accuracy: 0.9408595982561899\n",
            "Training loss: 0.8015322384934772, accuracy: 0.9419060466857904\n",
            "Training loss: 0.7839278224649464, accuracy: 0.9428913660864358\n",
            "Training loss: 0.7665394347333399, accuracy: 0.9438566295889151\n",
            "Training loss: 0.7497097249190832, accuracy: 0.9447589254121612\n",
            "Training loss: 0.7342448889516121, accuracy: 0.9454918306053701\n",
            "Training loss: 0.719907465375888, accuracy: 0.9460867662160569\n",
            "Training loss: 0.7051858492543764, accuracy: 0.9467448744923423\n",
            "Training loss: 0.6908416876349924, accuracy: 0.9473974892308991\n",
            "Training loss: 0.6773209206956573, accuracy: 0.9479935949599013\n",
            "Training loss: 0.6644436145185405, accuracy: 0.9485622604078101\n",
            "Training loss: 0.6518679775937443, accuracy: 0.9491438364442382\n",
            "Training loss: 0.6398335985256977, accuracy: 0.9497049387768542\n",
            "Training loss: 0.6286205084617995, accuracy: 0.9502065268673876\n",
            "Training loss: 0.618189209333771, accuracy: 0.9506556777047264\n",
            "Training loss: 0.6081276164759424, accuracy: 0.951095755375958\n",
            "Training loss: 0.5987489503738074, accuracy: 0.9514892060823005\n",
            "Training loss: 0.5896837299694074, accuracy: 0.9518857287352891\n",
            "Training loss: 0.5809984265928202, accuracy: 0.9522585367038787\n",
            "Training loss: 0.5725401248994058, accuracy: 0.952676023870863\n",
            "Validation loss: 0.2737637460231781, accuracy: 0.9614965095986039\n",
            "Validation loss: 0.22000122612172907, accuracy: 0.9686359669998413\n",
            "Validation loss: 0.22363552380175816, accuracy: 0.9683318789994183\n",
            "Validation loss: 0.21907566007106535, accuracy: 0.9688537972189383\n",
            "Validation loss: 0.2162216800015147, accuracy: 0.969214234027157\n",
            "Validation loss: 0.21667003923771427, accuracy: 0.9691403175580878\n",
            "Validation loss: 0.21411030189912827, accuracy: 0.9694750808228192\n",
            "Validation loss: 0.2125036596832141, accuracy: 0.9696940368212768\n",
            "Validation loss: 0.21164619830655462, accuracy: 0.9698064119966389\n",
            "Validation loss: 0.21148231674681653, accuracy: 0.9697754252728075\n",
            "Validation loss: 0.21229899947596068, accuracy: 0.9696868574291985\n",
            "Validation loss: 0.21226977778447642, accuracy: 0.9696751725547537\n"
          ]
        }
      ],
      "source": [
        "# train the model and evalutate it with tensorboard\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter()\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    epoch_correct = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        word_ids, char_ids = batch\n",
        "        word_ids = word_ids.to(device)\n",
        "        char_ids = char_ids.to(device)\n",
        "\n",
        "        logits = model(char_ids)\n",
        "\n",
        "        loss = criterion(logits.view(-1, len(vocab)), word_ids.view(-1))\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        avg_loss = total_loss / (i+1)\n",
        "        \n",
        "        epoch_correct += (logits.argmax(dim=2) == word_ids).sum().item()\n",
        "        epoch_acc = epoch_correct / (word_ids.size(0) * word_ids.size(1) * (i+1))\n",
        "        \n",
        "        \n",
        "        writer.add_scalar('Loss/train', avg_loss, i)\n",
        "        writer.add_scalar('Accuracy/train', epoch_acc, i)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f'Training loss: {avg_loss}, accuracy: {epoch_acc}')\n",
        "\n",
        "    return total_loss / len(iterator), epoch_acc\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    epoch_correct = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            word_ids, char_ids = batch\n",
        "            \n",
        "            word_ids = word_ids.to(device)\n",
        "            char_ids = char_ids.to(device)\n",
        "\n",
        "            logits = model(char_ids)\n",
        "\n",
        "            loss = criterion(logits.view(-1, len(vocab)), word_ids.view(-1))\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            avg_loss = total_loss / (i+1)\n",
        "            \n",
        "            epoch_correct += (logits.argmax(dim=2) == word_ids).sum().item()\n",
        "            epoch_acc = epoch_correct / (word_ids.size(0) * word_ids.size(1) * (i+1))\n",
        "            \n",
        "            \n",
        "            writer.add_scalar('Loss/test', avg_loss, i)\n",
        "            writer.add_scalar('Accuracy/test', epoch_acc, i)\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f'Validation loss: {avg_loss}, accuracy: {epoch_acc}')\n",
        "\n",
        "    return total_loss / len(iterator), epoch_acc\n",
        "\n",
        "N_EPOCHS = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "        \n",
        "        print(f'=== EPOCH {epoch} ===')\n",
        "\n",
        "        train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "        valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
        "        \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ4HsuafA5sQ"
      },
      "source": [
        "## Evaluate your embeddings model on IMDB movie reviews dataset (sentiment analysis) \n",
        "[Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "\n",
        "Preprocess data\n",
        "\n",
        "Disable training for ELMo, it will produce 5 embeddings for each word, add trainable parameters $\\gamma^{task}$ and $s^{task}_j$\n",
        "\n",
        "Don't forget metric plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IQ_0LTQf81CM"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      One of the other reviewers has mentioned that ...  positive\n",
              "1      A wonderful little production. <br /><br />The...  positive\n",
              "2      I thought this was a wonderful way to spend ti...  positive\n",
              "3      Basically there's a family where a little boy ...  negative\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...  positive\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  I am a Catholic taught in parochial elementary...  negative\n",
              "49998  I'm going to have to disagree with the previou...  negative\n",
              "49999  No one expects the Star Trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read IMDB dataset\n",
        "import pandas as pd\n",
        "\n",
        "imdb = pd.read_csv('IMDB Dataset.csv')\n",
        "imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter non-ASCII Strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        One of the other reviewers has mentioned that ...\n",
              "1        A wonderful little production. <br /><br />The...\n",
              "2        I thought this was a wonderful way to spend ti...\n",
              "3        Basically there's a family where a little boy ...\n",
              "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
              "                               ...                        \n",
              "45335    I thought this movie did a down right good job...\n",
              "45336    Bad plot, bad dialogue, bad acting, idiotic di...\n",
              "45337    I am a Catholic taught in parochial elementary...\n",
              "45338    I'm going to have to disagree with the previou...\n",
              "45339    No one expects the Star Trek movies to be high...\n",
              "Name: review, Length: 45340, dtype: object"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews = imdb['review']\n",
        "labels = imdb['sentiment']\n",
        "\n",
        "ascii_review_indices = np.array(list(map(lambda x: x.isascii(), reviews)))\n",
        "ascii_reviews = reviews[ascii_review_indices]\n",
        "ascii_reviews = ascii_reviews.reset_index(drop=True)\n",
        "ascii_reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train-Test Indices (80/20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split imdb into train and test sets with numpy\n",
        "np.random.seed(42)\n",
        "train_indices = np.random.choice(ascii_reviews.index, size=int(0.8*len(ascii_reviews)), replace=False)\n",
        "test_indices = ascii_reviews.index.difference(train_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Padded Char Ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([45340, 573, 35])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_review_char_ids = torch.full(\n",
        "    size=(len(ascii_reviews), max_num_words + 2, max_num_letters),\n",
        "    fill_value=symbols_vocab['<pad>']\n",
        ")\n",
        "padded_review_char_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "for sent_num, sent in enumerate(ascii_reviews):\n",
        "    tokenized = tokenizer(sent)\n",
        "    for i in range(0, min(len(tokenized), max_num_words) + 2):\n",
        "        if i == 0:\n",
        "            padded_review_char_ids[sent_num, i, 0] = symbols_vocab['<bow>']\n",
        "            padded_review_char_ids[sent_num, i, 1] = symbols_vocab['<bos>']\n",
        "            padded_review_char_ids[sent_num, i, 2] = symbols_vocab['<eow>']\n",
        "            continue\n",
        "        elif i == len(tokenized) + 1:\n",
        "            padded_review_char_ids[sent_num, i, 0] = symbols_vocab['<bow>']\n",
        "            padded_review_char_ids[sent_num, i, 1] = symbols_vocab['<eos>']\n",
        "            padded_review_char_ids[sent_num, i, 2] = symbols_vocab['<eow>']\n",
        "            continue\n",
        "        word = tokenized[i - 1]\n",
        "        for letter_num, letter in enumerate(word):\n",
        "            if letter_num >= max_num_letters:\n",
        "                break\n",
        "            padded_review_char_ids[sent_num, i, letter_num] = symbols_vocab[letter]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        positive\n",
              "1        positive\n",
              "2        positive\n",
              "3        negative\n",
              "4        positive\n",
              "           ...   \n",
              "45335    positive\n",
              "45336    negative\n",
              "45337    negative\n",
              "45338    negative\n",
              "45339    negative\n",
              "Name: sentiment, Length: 45340, dtype: object"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = labels[ascii_review_indices]\n",
        "labels = labels.reset_index(drop=True)\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        1\n",
              "1        1\n",
              "2        1\n",
              "3        0\n",
              "4        1\n",
              "        ..\n",
              "45335    1\n",
              "45336    0\n",
              "45337    0\n",
              "45338    0\n",
              "45339    0\n",
              "Name: sentiment, Length: 45340, dtype: int64"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encode labels with pandas\n",
        "labels = labels.map({'positive': 1, 'negative': 0})\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 1,  ..., 0, 0, 0])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_tensor = torch.tensor(labels.values)\n",
        "labels_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([45340])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([45340, 1])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reshape labels_tensor to (batch_size, 1)\n",
        "labels_tensor = labels_tensor.view(-1, 1)\n",
        "labels_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [1],\n",
              "        [1],\n",
              "        ...,\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create train_imdb and test_imdb datasets\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "train_imdb = TensorDataset(padded_review_char_ids[train_indices], labels_tensor[train_indices])\n",
        "test_imdb = TensorDataset(padded_review_char_ids[test_indices], labels_tensor[test_indices])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create train_imdb and test_imdb iterators\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_imdb_iter = DataLoader(train_imdb, batch_size=16, shuffle=True)\n",
        "test_imdb_iter = DataLoader(test_imdb, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ELMo IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class ELMoIMDB(ELMo):\n",
        "    \n",
        "    def __init__(self, vocab_size=len(vocab), n_tokens=max_num_words, n_chars=max_num_letters):\n",
        "        super(ELMoIMDB, self).__init__(vocab_size, n_tokens, n_chars)\n",
        "\n",
        "        # load weights from ELMo model\n",
        "        self.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "        # freeze model parameters\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # task parameters\n",
        "        self.gamma = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "        self.s_0 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "        self.s_1 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "        self.s_2 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "\n",
        "        # create linear layer with 64 output features and relu activation\n",
        "        self.linear = nn.Linear(64, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # create linear layer for classification\n",
        "        self.classifier = nn.Linear(64, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        embedded = model.embed_input(x)\n",
        "        token_embedding = model.charCNN(embedded)\n",
        "        lstm_output1, (h1_n, c1_n) = model.lstm1(token_embedding)\n",
        "        lstm_output2, (h2_n, c2_n) = model.lstm2(lstm_output1)\n",
        "\n",
        "        h_0 = torch.cat((token_embedding, token_embedding), dim=1)\n",
        "\n",
        "        r = self.gamma * (self.s_0 * h_0 + self.s_1 * lstm_output1 + self.s_2 * lstm_output2)\n",
        "\n",
        "        r = r.view(-1, self.n_tokens + 2, 2 * self.elmo_output_size)\n",
        "\n",
        "        out = torch.sum(r, dim=1)\n",
        "\n",
        "        # pass out through linear layer with relu activation\n",
        "        out = self.linear(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # pass out through classifier\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create elmoimdb model\n",
        "elmoimdb = ELMoIMDB().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "cudnn RNN backward can only be called in training mode",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [55], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, y\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m     33\u001b[0m \u001b[39m# backpropagate loss\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     36\u001b[0m \u001b[39m# update model parameters\u001b[39;00m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\datapaf\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\datapaf\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: cudnn RNN backward can only be called in training mode"
          ]
        }
      ],
      "source": [
        "# train elmoimdb model on imdb dataloader\n",
        "\n",
        "# define loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.Adam(elmoimdb.parameters(), lr=0.001)\n",
        "\n",
        "# train model\n",
        "for epoch in range(1):\n",
        "\n",
        "    # # initialize loss\n",
        "    # epoch_loss = 0\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (x, y) in enumerate(train_imdb_iter):\n",
        "\n",
        "        # move data to device\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        # pass data through model\n",
        "        outputs = elmoimdb(x)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = loss_fn(outputs, y.float())\n",
        "\n",
        "        # backpropagate loss\n",
        "        loss.backward()\n",
        "\n",
        "        # update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # # add loss to epoch loss\n",
        "        # epoch_loss += loss.item()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 16 == 15:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # # calculate average epoch loss\n",
        "    # epoch_loss /= len(imdb_dl)\n",
        "\n",
        "    # # print epoch loss\n",
        "    # print(f'Epoch {epoch} loss: {epoch_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKhTvahJBcBI"
      },
      "source": [
        "## Compare the results with BERT embeddings\n",
        "you can choose other bert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCojr57Zov7t"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "454c98ae2731e865a69a0b883f25a1cfa6b0f63785a62bbc0572ffd435d4c747"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
